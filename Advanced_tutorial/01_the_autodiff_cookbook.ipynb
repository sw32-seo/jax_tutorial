{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_the_autodiff_cookbook",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jgnw9v7tSLl"
      },
      "source": [
        "# The Autodiff Cookbook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qXWqhk_ryqW"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CobHkfhitZdG"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "### Starting with <code>grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnRcAM9gtcyQ",
        "outputId": "51bac747-7303-4df5-bc8d-15b73b28a820"
      },
      "source": [
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.070650935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUFoDDkItoZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "914ef2a7-a803-42e1-ccd5-00b6f60dd99a"
      },
      "source": [
        "print(grad(grad(jnp.tanh))(2.0))\n",
        "print(grad(grad(grad(jnp.tanh)))(2.0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.13621888\n",
            "0.2526544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTtkrc7E30pL"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probatility of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "  return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "\n",
        "inputs = jnp.array([[0.52, 1.12, 0.77],\n",
        "                    [0.88, -1.08, 0.15],\n",
        "                    [0.52, 0.06, -1.30],\n",
        "                    [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])\n",
        "\n",
        "# Training loss is the negative log-likelihood of the training examples.\n",
        "def loss(W, b):\n",
        "  preds = predict(W, b, inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYKXNkn26BNO",
        "outputId": "573610aa-8ffd-4c71-a095-9ac1e8b5d03d"
      },
      "source": [
        "# Differentiate 'loss' with respect to the first positional argument:\n",
        "W_grad = grad(loss, argnums=0)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# Since argnums=0 is the default, this does the same thing:\n",
        "W_grad = grad(loss)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# But we can choose different values too, and drop the keyword:\n",
        "b_grad = grad(loss, 1)(W, b)\n",
        "print('b_grad', b_grad)\n",
        "\n",
        "# Including tuple values\n",
        "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
        "print('W_grad', W_grad)\n",
        "print('b_grad', b_grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "b_grad -0.29227248\n",
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "b_grad -0.29227248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4-2zSvY7FuX"
      },
      "source": [
        "### Differentiating with respect to nested lists, tuples, and dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMklD8Sg6vcO",
        "outputId": "873c1c06-1004-4550-f65c-9d65be0b8f66"
      },
      "source": [
        "def loss2(params_dict):\n",
        "  preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "print(grad(loss2)({'W': W, 'b': b}))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'W': DeviceArray([-0.16965583, -0.8774647 , -1.4901344 ], dtype=float32), 'b': DeviceArray(-0.29227248, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZaBlwPe-5tv"
      },
      "source": [
        "### Evaluate a function and its gradient using <code>value_and_grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9R9ZHQ_BN4",
        "outputId": "9c0cb706-7ca3-485c-93c0-d23e5cf4737b"
      },
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "loss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\n",
        "print('loss value', loss_value)\n",
        "print('loss value', loss(W, b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss value 3.051939\n",
            "loss value 3.051939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7-oCp7_wOZ"
      },
      "source": [
        "### Checking against numerical differences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIH-kUj_UJR",
        "outputId": "f39731a6-5fde-4ef6-9ae8-4d8b8b6d76a2"
      },
      "source": [
        "# Set a step size for finite differences calculations\n",
        "eps = 1e-4\n",
        "\n",
        "# Check b_grad with scalar finite differences\n",
        "b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\n",
        "print('b_grad_numerical', b_grad_numerical)\n",
        "print('b_grad_autodiff', grad(loss, 1)(W, b))\n",
        "\n",
        "# Check W_grad with finite differences in a random direction\n",
        "key, subkey = random.split(key)\n",
        "vec = random.normal(subkey, W.shape)\n",
        "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
        "W_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\n",
        "print('W_dirderiv_numerical', W_grad_numerical)\n",
        "print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b_grad_numerical -0.29563904\n",
            "b_grad_autodiff -0.29227248\n",
            "W_dirderiv_numerical -0.19788742\n",
            "W_dirderiv_autodiff -0.19909093\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PMX6_qvUASE7"
      },
      "source": [
        "from jax.test_util import check_grads\n",
        "check_grads(loss, (W, b), order=2) # check up to 2nd order derivatives"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBRjoIdjCjWp"
      },
      "source": [
        "### Hessian-vector products with <code>grad</code>-of-<code>grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmCEn6ViCdd-"
      },
      "source": [
        "def hvp(f, x, v):\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(s), v)(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV1SCqf4GWg7"
      },
      "source": [
        "### Jacobians and Hessians using <code>jacfwd</code> and <code>jacrev</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sID_NE4QGMPW",
        "outputId": "d40e901e-9cbd-437b-c768-7cb0604ea0bc"
      },
      "source": [
        "from jax import jacfwd, jacrev\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "J = jacfwd(f)(W)\n",
        "print(\"jacfwd result, with shape\", J.shape)\n",
        "print(J)\n",
        "\n",
        "J = jacrev(f)(W)\n",
        "print(\"jacrev result, with shape\", J.shape)\n",
        "print(J)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jacfwd result, with shape (4, 3)\n",
            "[[ 0.05981757  0.12883784  0.08857601]\n",
            " [ 0.04015914 -0.04928622  0.00684531]\n",
            " [ 0.12188288  0.01406341 -0.3047072 ]\n",
            " [ 0.00140429 -0.00472523  0.00263778]]\n",
            "jacrev result, with shape (4, 3)\n",
            "[[ 0.05981756  0.12883782  0.088576  ]\n",
            " [ 0.04015914 -0.04928622  0.00684531]\n",
            " [ 0.12188289  0.01406341 -0.3047072 ]\n",
            " [ 0.00140429 -0.00472523  0.00263778]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09rg7h2nG5hP",
        "outputId": "1cff6923-f9b5-4f9b-d111-8813dc24ceab"
      },
      "source": [
        "def predict_dict(params, inputs):\n",
        "  return predict(params['W'], params['b'], inputs)\n",
        "\n",
        "J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
        "for k, v in J_dict.items():\n",
        "  print(\"Jacobian from {} to logits is\".format(k))\n",
        "  print(v)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jacobian from W to logits is\n",
            "[[ 0.05981756  0.12883782  0.088576  ]\n",
            " [ 0.04015914 -0.04928622  0.00684531]\n",
            " [ 0.12188289  0.01406341 -0.3047072 ]\n",
            " [ 0.00140429 -0.00472523  0.00263778]]\n",
            "Jacobian from b to logits is\n",
            "[0.11503378 0.04563539 0.23439017 0.00189768]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb2IoL5SIDNN",
        "outputId": "10deefa7-df29-4fcb-bfda-bdbd23c794ee"
      },
      "source": [
        "def hessian(f):\n",
        "  return jacfwd(jacrev(f))\n",
        "\n",
        "H = hessian(f)(W)\n",
        "print(\"hessian, with shape\", H.shape)\n",
        "print(H)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hessian, with shape (4, 3, 3)\n",
            "[[[ 0.02285465  0.0492254   0.03384246]\n",
            "  [ 0.04922541  0.10602394  0.07289145]\n",
            "  [ 0.03384247  0.07289146  0.05011287]]\n",
            "\n",
            " [[-0.03195214  0.03921399 -0.00544639]\n",
            "  [ 0.03921399 -0.04812626  0.0066842 ]\n",
            "  [-0.00544639  0.0066842  -0.00092836]]\n",
            "\n",
            " [[-0.01583708 -0.00182736  0.03959271]\n",
            "  [-0.00182736 -0.00021085  0.00456839]\n",
            "  [ 0.03959271  0.00456839 -0.09898177]]\n",
            "\n",
            " [[-0.00103522  0.00348338 -0.00194454]\n",
            "  [ 0.00348338 -0.01172109  0.0065431 ]\n",
            "  [-0.00194453  0.0065431  -0.00365257]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFstJxHoInJS"
      },
      "source": [
        "## How it's made: two foundational autodiff functions\n",
        "\n",
        "### Jacobian-Vector Products (JVPs, aka forward-mode autodiff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5q-vvAeUJL0"
      },
      "source": [
        "#### JVPs in JAX code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUL1QxALIWIV"
      },
      "source": [
        "from jax import jvp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "key, subkey = random.split(key)\n",
        "v = random.normal(subkey, W.shape)\n",
        "\n",
        "# Push forward the vector 'v' along 'f' evaluated at 'W'\n",
        "y, u = jvp(f, (W,), (v,))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SRg4fdbZ-T"
      },
      "source": [
        "### Vector-Jacobian products (VJPs, aka reverse-mode autodiff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oWf-k5ajb8l"
      },
      "source": [
        "#### VJPs in JAX code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJf2aBA1acgy"
      },
      "source": [
        "from jax import vjp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "y, vjp_fun = vjp(f, W)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "u = random.normal(subkey, y.shape)\n",
        "\n",
        "# Pull back the covector 'u' along 'f' evaluated at 'W'\n",
        "v = vjp_fun(u)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkwB2QozlD0i"
      },
      "source": [
        "#### Vector-valued gradients with VJPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fGKH40pkWKp",
        "outputId": "b9ccca10-c3cc-43e5-8cd5-271a3ee02f4e"
      },
      "source": [
        "from jax import vjp\n",
        "\n",
        "def vgrad(f, x):\n",
        "  y, vjp_fn = vjp(f, x)\n",
        "  return vjp_fn(jnp.ones(y.shape))[0]\n",
        "\n",
        "print(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6. 6.]\n",
            " [6. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfRi3KezmUs9"
      },
      "source": [
        "### Hessian-vector products using both forward- and reverse-mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j08NuZile2B"
      },
      "source": [
        "def hvp(f, x, v):\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w22XkAn9r9u4"
      },
      "source": [
        "from jax import jvp, grad\n",
        "\n",
        "# forward-over-reverse\n",
        "def hvp(f, primals, tangents):\n",
        "  return jvp(grad(f), primals, tangents)[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJai_mmlsVcz",
        "outputId": "356130e8-f5e7-4ca1-c331-03ad5a3bc481"
      },
      "source": [
        "def f(X):\n",
        "  return jnp.sum(jnp.tanh(X)**2)\n",
        "\n",
        "key, subkey1, subkey2 = random.split(key, 3)\n",
        "X = random.normal(subkey1, (30, 40))\n",
        "V = random.normal(subkey2, (30, 40))\n",
        "\n",
        "ans1 = hvp(f, (X,), (V,))\n",
        "ans2 = jnp.tensordot(hessian(f)(X), V, 2)\n",
        "print(jnp.allclose(ans1, ans2, 1e-4, 1e-4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopUEu9rs9P6"
      },
      "source": [
        "# reverse-over-forward\n",
        "def hvp_revfwd(f, primals, tangents):\n",
        "  g = lambda primals: jvp(f, primals, tangents)[1]\n",
        "  return grad(g)(primals)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNRm8xautTFo"
      },
      "source": [
        "Thatâ€™s not quite as good, though, because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEHZkbMgtSXD",
        "outputId": "d1936e87-128b-4337-c747-7482923456ec"
      },
      "source": [
        "# reverse-over-reverse, only works for single arguments\n",
        "def hvp_revrev(f, primals, tangents):\n",
        "  x, = primals\n",
        "  v, = tangents\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
        "\n",
        "\n",
        "print(\"Forward over reverse\")\n",
        "%timeit -n10 -r3 hvp(f, (X,), (V,))\n",
        "print(\"Reverse over forward\")\n",
        "%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\n",
        "print(\"Reverse over reverse\")\n",
        "%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n",
        "\n",
        "print(\"Naive full Hessian materialization\")\n",
        "%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forward over reverse\n",
            "10 loops, best of 3: 9.34 ms per loop\n",
            "Reverse over forward\n",
            "10 loops, best of 3: 10.9 ms per loop\n",
            "Reverse over reverse\n",
            "10 loops, best of 3: 13.6 ms per loop\n",
            "Naive full Hessian materialization\n",
            "10 loops, best of 3: 22.3 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3L7V_QvpLJU"
      },
      "source": [
        "## Composing VJPs, JVPs, and <code>vmap</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_wV6RhQtYQR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a499db2b-3a0f-4f0e-f910-43eae58bae23"
      },
      "source": [
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "# Pull back the covectors 'm_i' along 'f', evaluated at 'W', for all 'i'.\n",
        "# First, use a list comprehension to loop over rows in the matrix M.\n",
        "def loop_mjp(f, x, M):\n",
        "  y, vjp_fun = vjp(f, x)\n",
        "  return jnp.vstack([vjp_fun(mi) for mi in M])\n",
        "\n",
        "# Now, use vmap to build a computation that does a single fast matrix-matrix\n",
        "# multiply, rather than an outer loop over vector-matrix multiplies.\n",
        "\n",
        "def vmap_mjp(f, x, M):\n",
        "  y, vjp_fun = vjp(f, x)\n",
        "  outs, = vmap(vjp_fun)(M)\n",
        "  return outs\n",
        "\n",
        "key = random.PRNGKey(0)\n",
        "num_covecs = 128\n",
        "U = random.normal(key, (num_covecs,) + y.shape)\n",
        "\n",
        "loop_vs = loop_mjp(f, W, M=U)\n",
        "print('None-vmapped Matrix-Jacobian product')\n",
        "%timeit -n10 -r3 loop_mjp(f, W, M=U)\n",
        "\n",
        "print('\\nVmapped Matrix-Jacobian product')\n",
        "vmap_vs = vmap_mjp(f, W, M=U)\n",
        "%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n",
        "\n",
        "assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "None-vmapped Matrix-Jacobian product\n",
            "10 loops, best of 3: 225 ms per loop\n",
            "\n",
            "Vmapped Matrix-Jacobian product\n",
            "10 loops, best of 3: 8.33 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zgPxH2tnrc0-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}