{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_the_autodiff_cookbook",
      "provenance": [],
      "authorship_tag": "ABX9TyMMogvGUI874lU+v7B+G15Y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sw32-seo/jax_tutorial/blob/main/01_the_autodiff_cookbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jgnw9v7tSLl"
      },
      "source": [
        "# The Autodiff Cookbook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qXWqhk_ryqW"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CobHkfhitZdG"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "### Starting with <code>grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnRcAM9gtcyQ",
        "outputId": "43d7e72c-c3e9-4178-9e66-40d28a70b093"
      },
      "source": [
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.070650935\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUFoDDkItoZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ff4a6f5-6d98-4a79-bb70-1d5c08f648bc"
      },
      "source": [
        "print(grad(grad(jnp.tanh))(2.0))\n",
        "print(grad(grad(grad(jnp.tanh)))(2.0))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.13621888\n",
            "0.2526544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTtkrc7E30pL"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probatility of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "  return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "\n",
        "inputs = jnp.array([[0.52, 1.12, 0.77],\n",
        "                    [0.88, -1.08, 0.15],\n",
        "                    [0.52, 0.06, -1.30],\n",
        "                    [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])\n",
        "\n",
        "# Training loss is the negative log-likelihood of the training examples.\n",
        "def loss(W, b):\n",
        "  preds = predict(W, b, inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ())"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYKXNkn26BNO",
        "outputId": "a70f8488-31da-43bd-d329-b992b9a54b19"
      },
      "source": [
        "# Differentiate 'loss' with respect to the first positional argument:\n",
        "W_grad = grad(loss, argnums=0)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# Since argnums=0 is the default, this does the same thing:\n",
        "W_grad = grad(loss)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# But we can choose different values too, and drop the keyword:\n",
        "b_grad = grad(loss, 1)(W, b)\n",
        "print('b_grad', b_grad)\n",
        "\n",
        "# Including tuple values\n",
        "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
        "print('W_grad', W_grad)\n",
        "print('b_grad', b_grad)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "b_grad -0.29227248\n",
            "W_grad [-0.16965583 -0.8774647  -1.4901344 ]\n",
            "b_grad -0.29227248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4-2zSvY7FuX"
      },
      "source": [
        "### Differentiating with respect to nested lists, tuples, and dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMklD8Sg6vcO",
        "outputId": "08402dea-9a54-4856-a959-018b7896c797"
      },
      "source": [
        "def loss2(params_dict):\n",
        "  preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "print(grad(loss2)({'W': W, 'b': b}))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'W': DeviceArray([-0.16965583, -0.8774647 , -1.4901344 ], dtype=float32), 'b': DeviceArray(-0.29227248, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZaBlwPe-5tv"
      },
      "source": [
        "### Evaluate a function and its gradient using <code>value_and_grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9R9ZHQ_BN4",
        "outputId": "7b8c997d-f4ba-4eab-9ec8-8e8a1ed94a91"
      },
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "loss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\n",
        "print('loss value', loss_value)\n",
        "print('loss value', loss(W, b))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss value 3.051939\n",
            "loss value 3.051939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HkIH-kUj_UJR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}