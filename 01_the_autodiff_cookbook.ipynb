{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "01_the_autodiff_cookbook",
      "provenance": [],
      "authorship_tag": "ABX9TyN+P0NvV4Wkdb4z+Jh6uykr"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Jgnw9v7tSLl"
      },
      "source": [
        "# The Autodiff Cookbook"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qXWqhk_ryqW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7aff36d-6a72-48d0-860f-1b6f82d517ec"
      },
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "key = random.PRNGKey(0)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CobHkfhitZdG"
      },
      "source": [
        "## Gradients\n",
        "\n",
        "### Starting with <code>grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fnRcAM9gtcyQ",
        "outputId": "5778225c-59dc-484f-8d57-d7a2775cc68f"
      },
      "source": [
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.070650816\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xUFoDDkItoZt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf50c0e8-619a-40b6-e8bb-15b560c2a5c7"
      },
      "source": [
        "print(grad(grad(jnp.tanh))(2.0))\n",
        "print(grad(grad(grad(jnp.tanh)))(2.0))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-0.13621868\n",
            "0.25265405\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTtkrc7E30pL"
      },
      "source": [
        "def sigmoid(x):\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probatility of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "  return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "\n",
        "inputs = jnp.array([[0.52, 1.12, 0.77],\n",
        "                    [0.88, -1.08, 0.15],\n",
        "                    [0.52, 0.06, -1.30],\n",
        "                    [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])\n",
        "\n",
        "# Training loss is the negative log-likelihood of the training examples.\n",
        "def loss(W, b):\n",
        "  preds = predict(W, b, inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wYKXNkn26BNO",
        "outputId": "41475ecf-6918-4811-dd22-dc9a4bd297ca"
      },
      "source": [
        "# Differentiate 'loss' with respect to the first positional argument:\n",
        "W_grad = grad(loss, argnums=0)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# Since argnums=0 is the default, this does the same thing:\n",
        "W_grad = grad(loss)(W, b)\n",
        "print('W_grad', W_grad)\n",
        "\n",
        "# But we can choose different values too, and drop the keyword:\n",
        "b_grad = grad(loss, 1)(W, b)\n",
        "print('b_grad', b_grad)\n",
        "\n",
        "# Including tuple values\n",
        "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
        "print('W_grad', W_grad)\n",
        "print('b_grad', b_grad)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W_grad [-0.16965576 -0.8774648  -1.4901345 ]\n",
            "W_grad [-0.16965576 -0.8774648  -1.4901345 ]\n",
            "b_grad -0.2922724\n",
            "W_grad [-0.16965576 -0.8774648  -1.4901345 ]\n",
            "b_grad -0.2922724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4-2zSvY7FuX"
      },
      "source": [
        "### Differentiating with respect to nested lists, tuples, and dicts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SMklD8Sg6vcO",
        "outputId": "8446433c-ec6e-418e-ef09-212e133f171d"
      },
      "source": [
        "def loss2(params_dict):\n",
        "  preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "print(grad(loss2)({'W': W, 'b': b}))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'W': DeviceArray([-0.16965576, -0.8774648 , -1.4901345 ], dtype=float32), 'b': DeviceArray(-0.2922724, dtype=float32)}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZaBlwPe-5tv"
      },
      "source": [
        "### Evaluate a function and its gradient using <code>value_and_grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9R9ZHQ_BN4",
        "outputId": "4e77f471-b839-4bed-d10e-6a0d4d473428"
      },
      "source": [
        "from jax import value_and_grad\n",
        "\n",
        "loss_value, Wb_grad = value_and_grad(loss, (0, 1))(W, b)\n",
        "print('loss value', loss_value)\n",
        "print('loss value', loss(W, b))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss value 3.0519395\n",
            "loss value 3.0519395\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lu7-oCp7_wOZ"
      },
      "source": [
        "### Checking against numerical differences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkIH-kUj_UJR",
        "outputId": "69c56879-fcaf-4588-f65e-7055bf64de59"
      },
      "source": [
        "# Set a step size for finite differences calculations\n",
        "eps = 1e-4\n",
        "\n",
        "# Check b_grad with scalar finite differences\n",
        "b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.) / eps)\n",
        "print('b_grad_numerical', b_grad_numerical)\n",
        "print('b_grad_autodiff', grad(loss, 1)(W, b))\n",
        "\n",
        "# Check W_grad with finite differences in a random direction\n",
        "key, subkey = random.split(key)\n",
        "vec = random.normal(subkey, W.shape)\n",
        "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
        "W_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\n",
        "print('W_dirderiv_numerical', W_grad_numerical)\n",
        "print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b_grad_numerical -30516.492\n",
            "b_grad_autodiff -0.2922724\n",
            "W_dirderiv_numerical -0.19788742\n",
            "W_dirderiv_autodiff -0.19909069\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMX6_qvUASE7",
        "outputId": "39045103-d6c6-4de4-f799-a6f3654043c7"
      },
      "source": [
        "from jax.test_util import check_grads\n",
        "check_grads(loss, (W, b), order=2) # check up to 2nd order derivatives"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBRjoIdjCjWp"
      },
      "source": [
        "### Hessian-vector products with <code>grad</code>-of-<code>grad</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FmCEn6ViCdd-"
      },
      "source": [
        "def hvp(f, x, v):\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(s), v)(x))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BV1SCqf4GWg7"
      },
      "source": [
        "### Jacobians and Hessians using <code>jacfwd</code> and <code>jacrev</code>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sID_NE4QGMPW",
        "outputId": "9ca9bd86-cb08-49b2-fbb5-f21457d9cdbe"
      },
      "source": [
        "from jax import jacfwd, jacrev\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "J = jacfwd(f)(W)\n",
        "print(\"jacfwd result, with shape\", J.shape)\n",
        "print(J)\n",
        "\n",
        "J = jacrev(f)(W)\n",
        "print(\"jacrev result, with shape\", J.shape)\n",
        "print(J)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "jacfwd result, with shape (4, 3)\n",
            "[[ 0.05981753  0.12883773  0.08857594]\n",
            " [ 0.04015911 -0.04928619  0.0068453 ]\n",
            " [ 0.12188288  0.01406341 -0.3047072 ]\n",
            " [ 0.00140426 -0.00472516  0.00263774]]\n",
            "jacrev result, with shape (4, 3)\n",
            "[[ 0.05981752  0.12883773  0.08857594]\n",
            " [ 0.04015911 -0.04928619  0.0068453 ]\n",
            " [ 0.12188289  0.01406341 -0.3047072 ]\n",
            " [ 0.00140426 -0.00472516  0.00263774]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09rg7h2nG5hP",
        "outputId": "297bae68-ef49-4da4-ca10-3f84c69c1022"
      },
      "source": [
        "def predict_dict(params, inputs):\n",
        "  return predict(params['W'], params['b'], inputs)\n",
        "\n",
        "J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
        "for k, v in J_dict.items():\n",
        "  print(\"Jacobian from {} to logits is\".format(k))\n",
        "  print(v)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Jacobian from W to logits is\n",
            "[[ 0.05981752  0.12883773  0.08857594]\n",
            " [ 0.04015911 -0.04928619  0.0068453 ]\n",
            " [ 0.12188289  0.01406341 -0.3047072 ]\n",
            " [ 0.00140426 -0.00472516  0.00263774]]\n",
            "Jacobian from b to logits is\n",
            "[0.11503369 0.04563536 0.23439017 0.00189765]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pb2IoL5SIDNN",
        "outputId": "5ccae2da-c7b5-49f4-fec9-347775d607cb"
      },
      "source": [
        "def hessian(f):\n",
        "  return jacfwd(jacrev(f))\n",
        "\n",
        "H = hessian(f)(W)\n",
        "print(\"hessian, with shape\", H.shape)\n",
        "print(H)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "hessian, with shape (4, 3, 3)\n",
            "[[[ 0.02285464  0.04922538  0.03384245]\n",
            "  [ 0.04922539  0.10602391  0.07289143]\n",
            "  [ 0.03384246  0.07289144  0.05011286]]\n",
            "\n",
            " [[-0.03195212  0.03921397 -0.00544638]\n",
            "  [ 0.03921397 -0.04812624  0.0066842 ]\n",
            "  [-0.00544638  0.0066842  -0.00092836]]\n",
            "\n",
            " [[-0.01583708 -0.00182736  0.03959271]\n",
            "  [-0.00182736 -0.00021085  0.00456839]\n",
            "  [ 0.03959271  0.00456839 -0.09898177]]\n",
            "\n",
            " [[-0.0010352   0.00348332 -0.0019445 ]\n",
            "  [ 0.00348332 -0.01172091  0.006543  ]\n",
            "  [-0.0019445   0.006543   -0.00365252]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFstJxHoInJS"
      },
      "source": [
        "## How it's made: two foundational autodiff functions\n",
        "\n",
        "### Jacobian-Vector Products (JVPs, aka forward-mode autodiff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5q-vvAeUJL0"
      },
      "source": [
        "#### JVPs in JAX code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUL1QxALIWIV"
      },
      "source": [
        "from jax import jvp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "key, subkey = random.split(key)\n",
        "v = random.normal(subkey, W.shape)\n",
        "\n",
        "# Push forward the vector 'v' along 'f' evaluated at 'W'\n",
        "y, u = jvp(f, (W,), (v,))"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6SRg4fdbZ-T"
      },
      "source": [
        "### Vector-Jacobian products (VJPs, aka reverse-mode autodiff)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oWf-k5ajb8l"
      },
      "source": [
        "#### VJPs in JAX code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJf2aBA1acgy"
      },
      "source": [
        "from jax import vjp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "y, vjp_fun = vjp(f, W)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "u = random.normal(subkey, y.shape)\n",
        "\n",
        "# Pull back the covector 'u' along 'f' evaluated at 'W'\n",
        "v = vjp_fun(u)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkwB2QozlD0i"
      },
      "source": [
        "#### Vector-valued gradients with VJPs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fGKH40pkWKp",
        "outputId": "c40a2a19-1988-4d53-9d29-39022e53c17c"
      },
      "source": [
        "from jax import vjp\n",
        "\n",
        "def vgrad(f, x):\n",
        "  y, vjp_fn = vjp(f, x)\n",
        "  return vjp_fn(jnp.ones(y.shape))[0]\n",
        "\n",
        "print(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[6. 6.]\n",
            " [6. 6.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kfRi3KezmUs9"
      },
      "source": [
        "### Hessian-vector products using both forward- and reverse-mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j08NuZile2B"
      },
      "source": [
        "def hvp(f, x, v):\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w22XkAn9r9u4"
      },
      "source": [
        "from jax import jvp, grad\n",
        "\n",
        "# forward-over-reverse\n",
        "def hvp(f, primals, tangents):\n",
        "  return jvp(grad(f), primals, tangents)[1]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJai_mmlsVcz",
        "outputId": "82ca301f-d4d0-4317-f19c-d9a3a31be885"
      },
      "source": [
        "def f(X):\n",
        "  return jnp.sum(jnp.tanh(X)**2)\n",
        "\n",
        "key, subkey1, subkey2 = random.split(key, 3)\n",
        "X = random.normal(subkey1, (30, 40))\n",
        "V = random.normal(subkey2, (30, 40))\n",
        "\n",
        "ans1 = hvp(f, (X,), (V,))\n",
        "ans2 = jnp.tensordot(hessian(f)(X), V, 2)\n",
        "print(jnp.allclose(ans1, ans2, 1e-4, 1e-4))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NopUEu9rs9P6"
      },
      "source": [
        "# reverse-over-forward\n",
        "def hvp_revfwd(f, primals, tangents):\n",
        "  g = lambda primals: jvp(f, primals, tangents)[1]\n",
        "  return grad(g)(primals)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNRm8xautTFo"
      },
      "source": [
        "Thatâ€™s not quite as good, though, because forward-mode has less overhead than reverse-mode, and since the outer differentiation operator here has to differentiate a larger computation than the inner one, keeping forward-mode on the outside works best:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEHZkbMgtSXD",
        "outputId": "99b34167-064a-42e5-9dbc-f027a4fbd044"
      },
      "source": [
        "# reverse-over-reverse, only works for single arguments\n",
        "def hvp_revrev(f, primals, tangents):\n",
        "  x, = primals\n",
        "  v, = tangents\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
        "\n",
        "\n",
        "print(\"Forward over reverse\")\n",
        "%timeit -n10 -r3 hvp(f, (X,), (V,))\n",
        "print(\"Reverse over forward\")\n",
        "%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\n",
        "print(\"Reverse over reverse\")\n",
        "%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n",
        "\n",
        "print(\"Naive full Hessian materialization\")\n",
        "%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Forward over reverse\n",
            "10 loops, best of 3: 9.39 ms per loop\n",
            "Reverse over forward\n",
            "10 loops, best of 3: 11.6 ms per loop\n",
            "Reverse over reverse\n",
            "10 loops, best of 3: 14 ms per loop\n",
            "Naive full Hessian materialization\n",
            "10 loops, best of 3: 58.8 ms per loop\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_wV6RhQtYQR"
      },
      "source": [
        ""
      ],
      "execution_count": 21,
      "outputs": []
    }
  ]
}